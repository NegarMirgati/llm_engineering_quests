{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NegarMirgati/llm_engineering_quests/blob/main/week3/Week_3_exercise_synthetic_data_generation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Create meeting minutes from an Audio file\n",
        "\n",
        "I downloaded some Denver City Council meeting minutes and selected a portion of the meeting for us to transcribe. You can download it here:  \n",
        "https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n",
        "\n",
        "If you'd rather work with the original data, the HuggingFace dataset is [here](https://huggingface.co/datasets/huuuyeah/meetingbank) and the audio can be downloaded [here](https://huggingface.co/datasets/huuuyeah/MeetingBank_Audio/tree/main).\n",
        "\n",
        "The goal of this product is to use the Audio to generate meeting minutes, including actions.\n",
        "\n",
        "For this project, you can either use the Denver meeting minutes, or you can record something of your own!\n",
        "\n",
        "## Please note:\n",
        "\n",
        "When you run the pip installs in the first cell below, you might get this error - it can be safely ignored - it sounds quite severe, but it doesn't seem to affect anything else in this project!\n",
        "\n",
        "\n",
        "> ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
        "gcsfs 2024.10.0 requires fsspec==2024.10.0, but you have fsspec 2024.9.0 which is incompatible.\n",
        "\n"
      ],
      "metadata": {
        "id": "It89APiAtTUF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q requests torch transformers>=4.45.1 sentencepiece accelerate openai httpx==0.27.2 gradio bitsandbytes"
      ],
      "metadata": {
        "id": "f2vvgnFpHpID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U bitsandbytes"
      ],
      "metadata": {
        "id": "BfTA0fLRoxaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FW8nl3XRFrz0"
      },
      "outputs": [],
      "source": [
        "# imports\n",
        "\n",
        "import os\n",
        "import requests\n",
        "from IPython.display import Markdown, display, update_display\n",
        "from openai import OpenAI\n",
        "from google.colab import drive\n",
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, TextStreamer, BitsAndBytesConfig\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants\n",
        "model_mapping = {\n",
        "\n",
        "\"LLAMA\" :  \"meta-llama/Meta-Llama-3.1-8B-Instruct\",\n",
        "\"PHI3\" : \"microsoft/Phi-3-mini-4k-instruct\",\n",
        "\"QWEN2\" : \"Qwen/Qwen2-7B-Instruct\",\n",
        "}\n"
      ],
      "metadata": {
        "id": "q3D1_T0uG_Qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download denver_extract.mp3\n",
        "\n",
        "You can either use the same file as me, the extract from Denver city council minutes, or you can try your own..\n",
        "\n",
        "If you want to use the same as me, then please download my extract here, and put this on your Google Drive:  \n",
        "https://drive.google.com/file/d/1N_kpSojRR5RYzupz6nqM8hMSoEF_R7pU/view?usp=sharing\n"
      ],
      "metadata": {
        "id": "HTl3mcjyzIEE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to HuggingFace Hub\n",
        "\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token, add_to_git_credential=True)"
      ],
      "metadata": {
        "id": "xYW8kQYtF-3L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Sign in to OpenAI using Secrets in Colab\n",
        "\n",
        "openai_api_key = userdata.get('OPENAI_API_KEY')\n",
        "openai = OpenAI(api_key=openai_api_key)"
      ],
      "metadata": {
        "id": "qP6OB2OeGC2C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "quant_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")"
      ],
      "metadata": {
        "id": "UcRKUgcxMew6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_cache = {}"
      ],
      "metadata": {
        "id": "By6pSLsnrId3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_model(model_key):\n",
        "    if model_key in model_cache:\n",
        "        return model_cache[model_key]\n",
        "\n",
        "    # Clear cache if another model is already loaded (to stay under 12GB)\n",
        "    model_cache.clear()\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "    model_id = model_mapping[model_key]\n",
        "\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    quant_config = BitsAndBytesConfig(\n",
        "        load_in_4bit=True,\n",
        "        bnb_4bit_use_double_quant=True,\n",
        "        bnb_4bit_compute_dtype=torch.bfloat16,\n",
        "        bnb_4bit_quant_type=\"nf4\"\n",
        "    )\n",
        "\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_id,\n",
        "        device_map=\"auto\",\n",
        "        quantization_config=quant_config\n",
        "    )\n",
        "\n",
        "    model_cache[model_key] = (model, tokenizer)\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "-C5vSLbTrEOA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "load_model(\"LLAMA\")"
      ],
      "metadata": {
        "id": "K4znPdnt3Zxu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# More involved Gradio code as we're not using the preset Chat interface!\n",
        "# Passing in inbrowser=True in the last line will cause a Gradio window to pop up immediately.\n",
        "import gradio as gr\n",
        "import re\n",
        "\n",
        "\n",
        "def get_reviews(dataset_size, model_name):\n",
        "    try:\n",
        "      system_ptompt = f\"\"\"Please generate detailed book reviews from provided book information, including summary, key themes, strengths, weaknesses, and genre analysis. \\\n",
        "      Respond only with valid JSON, in a list. Do not write an introduction or summary or include roles like assistant in the output. \\\n",
        "      Note that I don't want code or anything. Just a list containing book info in JSON format for {dataset_size} random books.\\\n",
        "      Make sure if you are generating more than one JSON, put them in a LIST, separated by comma. \"\"\"\n",
        "      user_prompt = f\"\"\"Below are a few example entries of a dataset about books. \\\n",
        "      I want you to generate more synthetic data like this so that I can have a dataset of {dataset_size} entries like these. \\\n",
        "      Please generate more synthetic data following this format and without including the above instructions in the output. \\\n",
        "      Respond only with valid JSON. Do not write an introduction or summary. \\\n",
        "      Here is one example of a dataset containing two instances: \\\n",
        "        [ {{\n",
        "        \"title\": \"The Silent Echo\",\n",
        "        \"author\": \"Emily Watson\",\n",
        "        \"year_of_release\": 2021,\n",
        "        \"genre\": \"Mystery\",\n",
        "        \"avg_rating\": 4.3,\n",
        "        \"num_ratings\": 1245,\n",
        "        \"summary\": \"In the quiet town of Willow Creek, a series of mysterious disappearances \\\n",
        "        leaves the locals on edge. Detective Sarah Hayes, known for her sharp instincts, is called \\\n",
        "        in to unravel the case. As she digs deeper into the hidden secrets of the town, she discovers \\\n",
        "        a chilling connection between the victims and a decades-old unsolved crime. With each step closer \\\n",
        "        to the truth, Sarah finds herself caught in a dangerous game of cat and mouse, where nothing is as it \\\n",
        "        seems and the past refuses to stay buried.\"}} \\\n",
        "        , {{\n",
        "        \"title\": \"The Lost Garden\",\n",
        "        \"author\": \"David Miller\",\n",
        "        \"year_of_release\": 2018,\n",
        "        \"genre\": \"Historical Fiction\",\n",
        "        \"avg_rating\": 4.7,\n",
        "        \"num_ratings\": 3221,\n",
        "        \"summary\": \"Set against the backdrop of World War II, 'The Lost Garden' tells the story of Lily Adams, \\\n",
        "        a young British woman who volunteers as a gardener for the Women’s Land Army. While working on a secluded \\\n",
        "        estate in the English countryside, she uncovers a hidden garden that has been abandoned for years. As she \\\n",
        "        brings the garden back to life, she begins to uncover secrets about the estate's mysterious past, including \\\n",
        "        a love story that could change her own future forever. Through the garden's blooms, Lily learns the power \\\n",
        "        of resilience, love, and the healing that nature can bring.\"}} ]\"\"\"\n",
        "      messages = [\n",
        "        {\"role\": \"system\", \"content\": system_ptompt},\n",
        "        {\"role\": \"user\", \"content\": user_prompt}\n",
        "      ]\n",
        "      model, tokenizer = load_model(model_name)\n",
        "      input = tokenizer.apply_chat_template(messages, return_tensors=\"pt\", add_generation_prompt=True).to(\"cuda\")\n",
        "      streamer = TextStreamer(tokenizer)\n",
        "      output = model.generate(input, max_new_tokens=2000, streamer=streamer)\n",
        "      response = tokenizer.batch_decode(\n",
        "      output[:, input.shape[1]:],\n",
        "      skip_special_tokens = True)\n",
        "      cleaned_text = re.sub(r'<\\|.*?\\|>', '', response[0]).strip()\n",
        "      del model, tokenizer, input, output\n",
        "      torch.cuda.empty_cache()\n",
        "      return cleaned_text\n",
        "    except Exception as e:\n",
        "      print(f\"Error: {e}\")"
      ],
      "metadata": {
        "id": "XMEQkcHZj2Xu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_reviews(2, \"LLAMA\")"
      ],
      "metadata": {
        "id": "OGatLomnnrFD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "  with gr.Row():\n",
        "      dataset_size = gr.Textbox(label=\"Dataset Size:\")\n",
        "  with gr.Row():\n",
        "      model_name = gr.Dropdown(\n",
        "            [\"LLAMA\", \"PHI3\", \"QWEN2\"], label=\"Model:\")\n",
        "\n",
        "  with gr.Row():\n",
        "      submit_btn = gr.Button(\"Get Dataset\")\n",
        "  with gr.Row():\n",
        "      output = gr.JSON(label=\"Dataset:\")\n",
        "      submit_btn.click(fn=get_reviews, inputs=[dataset_size, model_name], outputs=output)\n",
        "\n",
        "demo.launch(inbrowser=True)"
      ],
      "metadata": {
        "id": "vWcTp-4HNGOY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}