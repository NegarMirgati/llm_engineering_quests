{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fe12c203-e6a6-452c-a655-afb8a03a4ff5",
   "metadata": {},
   "source": [
    "# End of week 1 exercise\n",
    "\n",
    "To demonstrate your familiarity with OpenAI API, and also Ollama, build a tool that takes a technical question,  \n",
    "and responds with an explanation. This is a tool that you will be able to use yourself during the course!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c1070317-3ed9-4659-abe3-828943230e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import ollama\n",
    "from openai import OpenAI\n",
    "from IPython.display import Markdown, display\n",
    "from dotenv import load_dotenv\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a456906-915a-4bfd-bb9d-57e505c5093f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constants\n",
    "\n",
    "MODEL_GPT = 'gpt-4o-mini'\n",
    "MODEL_LLAMA = 'llama3.2'\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8d7923c-5f28-4c30-8556-342d7c8497c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3f0d0137-52b0-47a8-81a8-11a90a010798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is the question; type over this to ask something new\n",
    "\n",
    "question = \"\"\"\n",
    "Please explain what this code does and why:\n",
    "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8f7c8ea8-4082-4ad0-8751-3301adcf6538",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25lpulling manifest ⠋ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠙ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠹ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠸ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest ⠼ \u001b[?25h\u001b[?25l\u001b[2K\u001b[1Gpulling manifest \n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \n",
      "verifying sha256 digest \n",
      "writing manifest \n",
      "success \u001b[?25h\n"
     ]
    }
   ],
   "source": [
    "# Get Llama 3.2 to answer\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "723db644-eece-4e04-a46f-d23770eec00d",
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = \"\"\"You are a knowledgeable assistant that can receive various technical questions, \\\n",
    "                 and respond to those questions in a straightforward yet meticulous manner. \\ \n",
    "                 Provide a reasonable explanation for why you think the answer is what you are providing.\\ \n",
    "                 Try to be thorough in your answers, and avoid hallucinating. Provide your answer in Markdown.\n",
    "                \"\"\"\n",
    "\n",
    "user_prompt = f\"\"\"Here is a technical question {question} \\\n",
    "                  Can you answer it and provide an explanation as well?\n",
    "                \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0aaa3da2-b51b-41dc-a8ef-e54abb25033b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "983d7a32-8377-492a-b736-30b0f432203a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Code Explanation**\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "This code is written in Python and utilizes the `yield from` statement, which was introduced in Python 3.3.\n",
       "\n",
       "### Breakdown:\n",
       "\n",
       "*   `for book in books`: This part iterates over a collection of objects (likely a list or dictionary) named `books`.\n",
       "*   `if book.get(\"author\")`: This condition filters out any book that does not have an \"author\" key.\n",
       "*   `{book.get(\"author\") for book in books if book.get(\"author\")}`: This is a generator expression that yields the value of the \"author\" key for each filtered book.\n",
       "\n",
       "### The `yield from` statement:\n",
       "\n",
       "The `yield from` statement is used to delegate a sub-generator's iteration to the surrounding function. In this case, it's being used in conjunction with the `yield` keyword to create an iterator.\n",
       "\n",
       "When you use `yield`, Python stops executing the function and remembers the state (i.e., the values of local variables) when the function yields. When the function is called again, Python resumes execution from where it left off.\n",
       "\n",
       "In contrast, `yield from` essentially \"forwards\" control to another generator or iterator. The sub-generator is responsible for creating its own iteration, and `yield from` simply delegates that to the surrounding function's main generator expression.\n",
       "\n",
       "### Final Result:\n",
       "\n",
       "The entire code generates a sequence of authors, one for each book in the `books` collection that has an \"author\" key.\n",
       "\n",
       "### Example Use Case\n",
       "\n",
       "Let's say you have a list of books with their respective authors. You can use this code to extract all unique author names from the books:\n",
       "\n",
       "```python\n",
       "# Define a dictionary of books with 'author' keys\n",
       "books = [\n",
       "    {\"title\": \"Book 1\", \"author\": \"Author A\"},\n",
       "    {\"title\": \"Book 2\", \"author\": \"Author B\"},\n",
       "    {\"title\": \"Book 3\", \"author\": None},\n",
       "]\n",
       "\n",
       "unique_authors = yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "print(unique_authors)  # Output: [\"Author A\", \"Author B\"]\n",
       "```\n",
       "\n",
       "In this example, the code filters out any book without an author and yields only the unique author names."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "response = ollama.chat(model=MODEL_LLAMA, messages=messages)\n",
    "display(Markdown(response['message']['content']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "60ce7000-a4a5-4cce-a261-e75ef45063b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "API key found and looks good so far!\n"
     ]
    }
   ],
   "source": [
    "# Get gpt-4o-mini to answer, with streaming\n",
    "\n",
    "## Load environment variables in a file called .env\n",
    "\n",
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found - please head over to the troubleshooting notebook in this folder to identify & fix!\")\n",
    "elif not api_key.startswith(\"sk-proj-\"):\n",
    "    print(\"An API key was found, but it doesn't start sk-proj-; please check you're using the right key - see troubleshooting notebook\")\n",
    "elif api_key.strip() != api_key:\n",
    "    print(\"An API key was found, but it looks like it might have space or tab characters at the start or end - please remove them - see troubleshooting notebook\")\n",
    "else:\n",
    "    print(\"API key found and looks good so far!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8de5c74d-8881-4aa7-91e7-f9cf9f527d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()\n",
    "\n",
    "# If this doesn't work, try Kernel menu >> Restart Kernel and Clear Outputs Of All Cells, then run the cells from the top of this notebook down.\n",
    "# If it STILL doesn't work (horrors!) then please see the Troubleshooting notebook in this folder for full instructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7d326aab-aa9c-4088-8b40-29b0de366947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Certainly! Let's break down the code you provided:\n",
       "\n",
       "```python\n",
       "yield from {book.get(\"author\") for book in books if book.get(\"author\")}\n",
       "```\n",
       "\n",
       "### Explanation\n",
       "\n",
       "1. **Context of the Code**:\n",
       "   - The code snippet is likely part of a generator function (though we don’t have the surrounding context). The use of `yield` suggests that this function produces a series of results one at a time, as opposed to returning a single value.\n",
       "\n",
       "2. **Set Comprehension**:\n",
       "   - The expression `{book.get(\"author\") for book in books if book.get(\"author\")}` is a set comprehension. It creates a set of authors from the list (or iterable) `books`.\n",
       "   - This comprehension does the following:\n",
       "     - Iterates over each `book` in the `books` collection.\n",
       "     - Uses `book.get(\"author\")` to safely attempt to retrieve the `\"author\"` value from each book dictionary. The `get()` method is preferred here because it returns `None` if the key `\"author\"` does not exist, rather than raising a KeyError.\n",
       "     - The condition `if book.get(\"author\")` filters out books that do not have an `\"author\"` value (i.e., it excludes books where the author is `None` or an empty string).\n",
       "\n",
       "3. **Set Behavior**:\n",
       "   - The curly braces `{ ... }` indicate that the result will be a set, which inherently ensures that all author names are unique. If multiple books share the same author, they will only appear once in the resulting set.\n",
       "\n",
       "4. **Yielding the Results**:\n",
       "   - The `yield from` statement is used to yield all items from the provided iterable (in this case, the set of authors) one by one.\n",
       "   - This means that the function will yield each unique author in the set, allowing the caller to iterate over them as if they were yielded individually.\n",
       "\n",
       "### Summary\n",
       "\n",
       "In summary, this line of code collects all the unique authors from a list of books and yields them one at a time. By using a set comprehension, it filters out any books without an author, ensuring that only those with a valid author name are included in the output. \n",
       "\n",
       "The main benefits of this approach include:\n",
       "- Ensures uniqueness of the authors.\n",
       "- Safe retrieval of author names with the `get()` method.\n",
       "- Efficiently allows for yielding items in a generator context. \n",
       "\n",
       "This is a common pattern for processing collections of dictionaries in Python, especially when you need to aggregate values while ensuring uniqueness and avoiding errors due to missing keys."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# To give you a preview -- calling OpenAI with these messages is this easy. Any problems, head over to the Troubleshooting notebook.\n",
    "response = openai.chat.completions.create(\n",
    "        model = MODEL_GPT,\n",
    "        messages = messages\n",
    "    )\n",
    "\n",
    "# Step 4: print the result\n",
    "display(Markdown(response.choices[0].message.content))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
